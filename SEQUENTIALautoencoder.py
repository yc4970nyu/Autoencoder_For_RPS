# -*- coding: utf-8 -*-
"""Autoencoder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lSsedNYsQ0A7R08S-VS_lUjL5UqhZDd4

# **Sequential Approach RPS Type 1**
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, UpSampling1D, Conv1DTranspose, Input, Flatten, Reshape
from tensorflow.keras.models import Model
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min
import os

from google.colab import drive
drive.mount('/content/drive')
file_path = '/content/drive/MyDrive/reshaped_final_df.csv'
df = pd.read_csv(file_path)
data_reshaped = df.values.reshape((df.shape[0], df.shape[1], 1))

'''Input Layer: This layer receives the input data. The shape (data_reshaped.shape[1], 1)
means it expects a sequence with length data_reshaped.shape[1] and 1 feature per time step.'''
# the encoder
input_layer = Input(shape=(data_reshaped.shape[1], 1))
x = Conv1D(32, kernel_size=3, activation='relu', padding='same')(input_layer)
x = MaxPooling1D(pool_size=2, padding='same')(x)
x = LSTM(16, return_sequences=True)(x)
encoded = LSTM(9, return_sequences=False)(x)

# the decoder
x = Dense(16, activation='relu')(encoded)
x = Dense(data_reshaped.shape[1] // 2 * 32, activation='relu')(x)
x = Reshape((data_reshaped.shape[1] // 2, 32))(x)
x = UpSampling1D(size=2)(x)
decoded = Conv1D(1, kernel_size=3, activation='linear', padding='same')(x)

# Combine encoder and decoder into an autoencoder
autoencoder = Model(inputs=input_layer, outputs=decoded)

encoder_model = Model(inputs=input_layer, outputs=encoded)
decoder_input = Input(shape=(9,))
x = Dense(16, activation='relu')(decoder_input)
x = Dense(data_reshaped.shape[1] // 2 * 32, activation='relu')(x)
x = Reshape((data_reshaped.shape[1] // 2, 32))(x)
x = UpSampling1D(size=2)(x)
decoder_output = Conv1D(1, kernel_size=3, activation='linear', padding='same')(x)
decoder_model = Model(inputs=decoder_input, outputs=decoder_output)

from sklearn.cluster import KMeans
import tensorflow as tf
import tensorflow.keras.backend as K
from tensorflow.keras.optimizers import Adam

"""Define and Train the Autoencoder Using the Autoencoder Loss"""

# Define autoencoder loss function (reconstruction loss)
def autoencoder_loss(y_true, y_pred):
    return tf.reduce_mean(tf.square(y_true - y_pred))

# Compile and train the autoencoder with reconstruction loss
autoencoder.compile(optimizer='adam', loss=autoencoder_loss)
history_autoencoder = autoencoder.fit(data_reshaped, data_reshaped, epochs=50, batch_size=32)

# Encode the data
encoded_data = encoder_model.predict(data_reshaped)

# Train the decoder model separately
decoder_model.compile(optimizer='adam', loss=autoencoder_loss)
history_decoder = decoder_model.fit(encoded_data, data_reshaped, epochs=50, batch_size=32)



# Plot training and validation loss
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(12, 6))
sns.set(style="whitegrid")
plt.plot(history_autoencoder.history['loss'], label='Train Loss', color='blue', lw=2)
plt.xlabel('Epoch', fontsize=14)
plt.ylabel('Loss', fontsize=14)
plt.title('Autoencoder Training Loss', fontsize=16)
plt.legend(fontsize=12)
plt.grid(True)
plt.show()

# Final loss values
final_train_loss = history_autoencoder.history['loss'][-1]
print(f"Final Training Loss: {final_train_loss:.6f}")

# Encode Data into Latent Space Using the Trained Encoder
#encoded_data = encoder_model.predict(data_reshaped)

encoded_data.shape

# Define function to calculate autoencoder loss for given k
def calculate_autoencoder_loss_for_k(encoded_data, original_data, k):
    # Perform KMeans clustering with k clusters
    kmeans = KMeans(n_clusters=k, random_state=0).fit(encoded_data)
    labels = kmeans.labels_
    centroids = kmeans.cluster_centers_

    # Replace each row vector with the closest point in the cluster to the centroid
    replaced_encoded_data = np.zeros_like(encoded_data)
    for i in range(k):
        cluster_points = encoded_data[labels == i]
        centroid = centroids[i]
        closest_point = cluster_points[np.argmin(np.sum(np.square(cluster_points - centroid), axis=1))]
        replaced_encoded_data[labels == i] = closest_point

    # Decode the imputed encoded data using the trained decoder model
    decoded_imputed_data = decoder_model.predict(replaced_encoded_data)

    # Calculate the autoencoder loss on the final reduced data
    final_autoencoder_loss = autoencoder_loss(original_data, decoded_imputed_data).numpy()

    return final_autoencoder_loss

# Loop over a range of k values and calculate autoencoder loss
k_values = range(10, 501, 10)
autoencoder_losses = []

for k in k_values:
    loss = calculate_autoencoder_loss_for_k(encoded_data, data_reshaped, k)
    autoencoder_losses.append(loss)
    print(f'k={k}, Autoencoder Loss={loss:.6f}')

# Plot the results
plt.figure(figsize=(10, 6))
plt.plot(k_values, autoencoder_losses, marker='o')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Autoencoder Loss')
plt.title('Autoencoder Loss vs. Number of Clusters (k)')
plt.grid(True)
plt.show()

from sklearn.cluster import KMeans

# Determine the best number of clusters k using the Elbow method (For simplicity, let's assume k=10 here)
k = 1000
kmeans = KMeans(n_clusters=k, random_state=0).fit(encoded_data)
labels = kmeans.labels_
centroids = kmeans.cluster_centers_

'''
import tensorflow as tf
import tensorflow.keras.backend as K
from tensorflow.keras.optimizers import Adam

# Convert centroids to a TensorFlow variable
centroids = tf.Variable(centroids, dtype=tf.float32)

# Clustering loss function
def clustering_loss(encoded_data, labels, centroids):
    clustering_loss_value = 0
    for i in range(len(encoded_data)):
        clustering_loss_value += K.sum(K.square(encoded_data[i] - centroids[labels[i]]))
    clustering_loss_value /= len(encoded_data)
    return clustering_loss_value

# Custom training loop to minimize clustering loss
optimizer = Adam()
epochs = 50
batch_size = 32
steps_per_epoch = len(encoded_data) // batch_size

clustering_losses = []

for epoch in range(epochs):
    epoch_loss = 0
    for step in range(steps_per_epoch):
        batch_indices = np.random.choice(len(encoded_data), batch_size, replace=False)
        batch_encoded_data = encoded_data[batch_indices]
        batch_labels = labels[batch_indices]
        with tf.GradientTape() as tape:
            batch_loss = clustering_loss(batch_encoded_data, batch_labels, centroids)
        grads = tape.gradient(batch_loss, [centroids])
        optimizer.apply_gradients(zip(grads, [centroids]))
        epoch_loss += batch_loss.numpy()
    clustering_losses.append(epoch_loss / steps_per_epoch)
    print(f'Epoch {epoch + 1}/{epochs}, Clustering Loss: {epoch_loss / steps_per_epoch}')

# Plot clustering training loss
plt.figure(figsize=(10, 6))
plt.plot(clustering_losses, label='Clustering Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Clustering Training Loss')
plt.show()

# Plot clustering training loss
plt.figure(figsize=(14, 8))
sns.set(style="whitegrid")
plt.plot(clustering_losses, label='Clustering Loss', color='purple', lw=2)
plt.xlabel('Epoch', fontsize=14)
plt.ylabel('Loss', fontsize=14)
plt.title('Clustering Training Loss', fontsize=16)
plt.legend(fontsize=12)
plt.grid(True)
plt.show()

# Final clustering loss value
final_clustering_loss = clustering_losses[-1]
print(f"Final Clustering Loss: {final_clustering_loss:.6f}")
'''

# Replace each row vector with the closest point in the cluster to the centroid
replaced_encoded_data = np.zeros_like(encoded_data)
for i in range(k):
    cluster_points = encoded_data[labels == i]
    centroid = centroids[i]
    closest_point = cluster_points[np.argmin(np.sum(np.square(cluster_points - centroid), axis=1))]
    replaced_encoded_data[labels == i] = closest_point

# Decode the imputed encoded data using the trained decoder model
decoded_imputed_data = decoder_model.predict(encoded_data)

decoded_imputed_data=autoencoder.predict(data_reshaped)

decoded_imputed_data.shape

reduced_df = decoded_imputed_data.reshape((7300,96))

# Save the reduced data to a CSV file
#reduced_df = pd.DataFrame(reduced_df)
#desktop_path = os.path.join("/content/drive/MyDrive/", "reduced_df.csv")
#reduced_df.to_csv(desktop_path, index=False)
#print(f"Reduced DataFrame saved to {desktop_path}")

# Assuming data_reshaped is a numpy array and reduced_df is also a numpy array
# Convert data_reshaped to a DataFrame for easier manipulation
input_df = pd.DataFrame(data_reshaped.reshape(data_reshaped.shape[0], -1))

# Function to visualize the input and output time series data for all samples
def visualize_full_reconstruction(input_data, reconstructed_data):
    plt.figure(figsize=(10, 6))
    plt.plot(input_data.to_numpy().flatten(), label='Input Data (Original)', color='orange', alpha=0.6)
    plt.plot(reconstructed_data.to_numpy().flatten(), label='Output Data (Reconstructed)', color='blue', alpha=0.6)
    plt.xlabel('Time')
    plt.ylabel('Value')
    plt.title('Sequential with K=1000: Input (Original) vs Output (Reconstructed)')
    plt.legend()
    plt.show()

# Convert reduced_df to a DataFrame
reduced_df = pd.DataFrame(reduced_df)

# Visualize the full input and output time series data
visualize_full_reconstruction(input_df, reduced_df)

# Function to visualize the input and output time series data for all samples
def visualize_full_reconstruction(input_data, reconstructed_data, num_features=96):
    fig, axes = plt.subplots(num_features, 1, figsize=(20, 2 * num_features), sharex=True)
    for j in range(num_features):
        ax = axes[j]
        ax.plot(input_data.iloc[:, j], label='Input (Original)', color='orange', alpha=0.6)
        ax.plot(reconstructed_data.iloc[:, j], label='Output (Reconstructed)', color='blue', alpha=0.6)
        ax.set_ylabel('Value')
        ax.set_title(f'Feature {j+1}')
        if j == 0:
            ax.legend()
    plt.xlabel('Samples')
    plt.tight_layout()
    plt.show()

# Visualize the full input and output time series data for all samples
visualize_full_reconstruction(input_df, reduced_df)







